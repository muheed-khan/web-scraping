{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/muheedahmedkhan/web-scraping-using-scrapy?scriptVersionId=193220967\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"!pip install scrapy","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:47:34.836939Z","iopub.execute_input":"2024-08-19T17:47:34.837388Z","iopub.status.idle":"2024-08-19T17:48:01.486751Z","shell.execute_reply.started":"2024-08-19T17:47:34.837349Z","shell.execute_reply":"2024-08-19T17:48:01.48529Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting scrapy\n  Downloading Scrapy-2.11.2-py2.py3-none-any.whl.metadata (5.3 kB)\nCollecting Twisted>=18.9.0 (from scrapy)\n  Downloading twisted-24.7.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: cryptography>=36.0.0 in /opt/conda/lib/python3.10/site-packages (from scrapy) (41.0.7)\nCollecting cssselect>=0.9.1 (from scrapy)\n  Downloading cssselect-1.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\nCollecting itemloaders>=1.0.1 (from scrapy)\n  Downloading itemloaders-1.3.1-py3-none-any.whl.metadata (3.9 kB)\nCollecting parsel>=1.5.0 (from scrapy)\n  Downloading parsel-1.9.1-py2.py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: pyOpenSSL>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from scrapy) (23.3.0)\nCollecting queuelib>=1.4.2 (from scrapy)\n  Downloading queuelib-1.7.0-py2.py3-none-any.whl.metadata (5.7 kB)\nCollecting service-identity>=18.1.0 (from scrapy)\n  Downloading service_identity-24.1.0-py3-none-any.whl.metadata (4.8 kB)\nCollecting w3lib>=1.17.0 (from scrapy)\n  Downloading w3lib-2.2.1-py3-none-any.whl.metadata (2.1 kB)\nCollecting zope.interface>=5.1.0 (from scrapy)\n  Downloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m612.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting protego>=0.1.15 (from scrapy)\n  Downloading Protego-0.3.1-py2.py3-none-any.whl.metadata (5.9 kB)\nCollecting itemadapter>=0.1.0 (from scrapy)\n  Downloading itemadapter-0.9.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from scrapy) (69.0.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from scrapy) (21.3)\nCollecting tldextract (from scrapy)\n  Downloading tldextract-5.1.2-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: lxml>=4.4.1 in /opt/conda/lib/python3.10/site-packages (from scrapy) (5.2.2)\nRequirement already satisfied: defusedxml>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from scrapy) (0.7.1)\nCollecting PyDispatcher>=2.0.5 (from scrapy)\n  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\nRequirement already satisfied: jmespath>=0.9.5 in /opt/conda/lib/python3.10/site-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (23.2.0)\nRequirement already satisfied: pyasn1 in /opt/conda/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (0.5.1)\nRequirement already satisfied: pyasn1-modules in /opt/conda/lib/python3.10/site-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\nCollecting automat>=0.8.0 (from Twisted>=18.9.0->scrapy)\n  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\nCollecting constantly>=15.1 (from Twisted>=18.9.0->scrapy)\n  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\nCollecting hyperlink>=17.1.1 (from Twisted>=18.9.0->scrapy)\n  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nCollecting incremental>=24.7.0 (from Twisted>=18.9.0->scrapy)\n  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\nRequirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from Twisted>=18.9.0->scrapy) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->scrapy) (3.1.1)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from tldextract->scrapy) (3.6)\nRequirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.10/site-packages (from tldextract->scrapy) (2.32.3)\nCollecting requests-file>=1.4 (from tldextract->scrapy)\n  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\nRequirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.10/site-packages (from tldextract->scrapy) (3.13.1)\nRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\nRequirement already satisfied: tomli in /opt/conda/lib/python3.10/site-packages (from incremental>=24.7.0->Twisted>=18.9.0->scrapy) (2.0.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2024.2.2)\nDownloading Scrapy-2.11.2-py2.py3-none-any.whl (290 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\nDownloading itemadapter-0.9.0-py3-none-any.whl (11 kB)\nDownloading itemloaders-1.3.1-py3-none-any.whl (12 kB)\nDownloading parsel-1.9.1-py2.py3-none-any.whl (17 kB)\nDownloading Protego-0.3.1-py2.py3-none-any.whl (8.5 kB)\nDownloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\nDownloading queuelib-1.7.0-py2.py3-none-any.whl (13 kB)\nDownloading service_identity-24.1.0-py3-none-any.whl (12 kB)\nDownloading twisted-24.7.0-py3-none-any.whl (3.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading w3lib-2.2.1-py3-none-any.whl (21 kB)\nDownloading zope.interface-7.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (254 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.1/254.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tldextract-5.1.2-py3-none-any.whl (97 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.6/97.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading Automat-24.8.1-py3-none-any.whl (42 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading constantly-23.10.4-py3-none-any.whl (13 kB)\nDownloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading incremental-24.7.2-py3-none-any.whl (20 kB)\nDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\nInstalling collected packages: PyDispatcher, zope.interface, w3lib, queuelib, protego, itemadapter, incremental, hyperlink, cssselect, constantly, automat, Twisted, requests-file, parsel, tldextract, service-identity, itemloaders, scrapy\nSuccessfully installed PyDispatcher-2.0.7 Twisted-24.7.0 automat-24.8.1 constantly-23.10.4 cssselect-1.2.0 hyperlink-21.0.0 incremental-24.7.2 itemadapter-0.9.0 itemloaders-1.3.1 parsel-1.9.1 protego-0.3.1 queuelib-1.7.0 requests-file-2.1.0 scrapy-2.11.2 service-identity-24.1.0 tldextract-5.1.2 w3lib-2.2.1 zope.interface-7.0.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from scrapy import Selector","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:50:02.371835Z","iopub.execute_input":"2024-08-19T17:50:02.372401Z","iopub.status.idle":"2024-08-19T17:50:03.08064Z","shell.execute_reply.started":"2024-08-19T17:50:02.372355Z","shell.execute_reply":"2024-08-19T17:50:03.079344Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scrapy/utils/_compression.py:15: ScrapyDeprecationWarning: You have brotlipy installed, and Scrapy will use it, but Scrapy support for brotlipy is deprecated and will stop working in a future version of Scrapy. brotlipy itself is deprecated, it has been superseded by brotlicffi (not currently supported by Scrapy). Please, uninstall brotlipy and install brotli instead. brotlipy has the same import name as brotli, so keeping both installed is strongly discouraged.\n  warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"html = '''\n<html>\n<body>\n\n         <div class=\"hello datacamp\">\n         <p>Hello World!</p>\n         <p>Enjoy DataCamp!</p>\n         </div>\n         \n\n</body>\n</html>\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.169401Z","iopub.execute_input":"2024-07-14T22:58:35.170039Z","iopub.status.idle":"2024-07-14T22:58:35.175663Z","shell.execute_reply.started":"2024-07-14T22:58:35.169998Z","shell.execute_reply":"2024-07-14T22:58:35.174407Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"sel = Selector(text = html)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.178885Z","iopub.execute_input":"2024-07-14T22:58:35.179339Z","iopub.status.idle":"2024-07-14T22:58:35.194052Z","shell.execute_reply.started":"2024-07-14T22:58:35.179298Z","shell.execute_reply":"2024-07-14T22:58:35.192758Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"xpath = '/html/body/div/p'\nsel.xpath(xpath)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.195573Z","iopub.execute_input":"2024-07-14T22:58:35.195926Z","iopub.status.idle":"2024-07-14T22:58:35.211502Z","shell.execute_reply.started":"2024-07-14T22:58:35.195898Z","shell.execute_reply":"2024-07-14T22:58:35.209987Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"[<Selector query='/html/body/div/p' data='<p>Hello World!</p>'>,\n <Selector query='/html/body/div/p' data='<p>Enjoy DataCamp!</p>'>]"},"metadata":{}}]},{"cell_type":"code","source":"sel.xpath(xpath).extract()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.213144Z","iopub.execute_input":"2024-07-14T22:58:35.213593Z","iopub.status.idle":"2024-07-14T22:58:35.222336Z","shell.execute_reply.started":"2024-07-14T22:58:35.213551Z","shell.execute_reply":"2024-07-14T22:58:35.221048Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"['<p>Hello World!</p>', '<p>Enjoy DataCamp!</p>']"},"metadata":{}}]},{"cell_type":"code","source":"sel.xpath(xpath).extract_first()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.223923Z","iopub.execute_input":"2024-07-14T22:58:35.224303Z","iopub.status.idle":"2024-07-14T22:58:35.236515Z","shell.execute_reply.started":"2024-07-14T22:58:35.224272Z","shell.execute_reply":"2024-07-14T22:58:35.235342Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"'<p>Hello World!</p>'"},"metadata":{}}]},{"cell_type":"markdown","source":"another way to do the same ting done above","metadata":{}},{"cell_type":"code","source":"ps = sel.xpath('//p')\nfirst_p = ps[0]\nfirst_p.extract()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.238192Z","iopub.execute_input":"2024-07-14T22:58:35.23874Z","iopub.status.idle":"2024-07-14T22:58:35.249212Z","shell.execute_reply.started":"2024-07-14T22:58:35.238697Z","shell.execute_reply":"2024-07-14T22:58:35.247689Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"'<p>Hello World!</p>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Request to get the web pages","metadata":{}},{"cell_type":"code","source":"import requests\nurl = 'https://en.wikipedia.org/wiki/Web_scraping'\nhtml = requests.get(url).content\nsel = Selector(text = html)","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.250581Z","iopub.execute_input":"2024-07-14T22:58:35.250932Z","iopub.status.idle":"2024-07-14T22:58:35.863867Z","shell.execute_reply.started":"2024-07-14T22:58:35.250902Z","shell.execute_reply":"2024-07-14T22:58:35.862789Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#first 5 p elements\nxpath = '//p'\nselected_text = sel.xpath(xpath)\nselected_text = selected_text[:5]\nselected_text","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.867602Z","iopub.execute_input":"2024-07-14T22:58:35.868554Z","iopub.status.idle":"2024-07-14T22:58:35.877957Z","shell.execute_reply.started":"2024-07-14T22:58:35.868519Z","shell.execute_reply":"2024-07-14T22:58:35.876541Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"[<Selector query='//p' data='<p><b>Web scraping</b>, <b>web harves...'>,\n <Selector query='//p' data='<p>Scraping a web page involves fetch...'>,\n <Selector query='//p' data='<p>As well as <a href=\"/wiki/Contact_...'>,\n <Selector query='//p' data='<p><a href=\"/wiki/Web_page\" title=\"We...'>,\n <Selector query='//p' data='<p>Newer forms of web scraping involv...'>]"},"metadata":{}}]},{"cell_type":"code","source":"selected_text[0].extract()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.879187Z","iopub.execute_input":"2024-07-14T22:58:35.879533Z","iopub.status.idle":"2024-07-14T22:58:35.889548Z","shell.execute_reply.started":"2024-07-14T22:58:35.8795Z","shell.execute_reply":"2024-07-14T22:58:35.888254Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'<p><b>Web scraping</b>, <b>web harvesting</b>, or <b>web data extraction</b> is <a href=\"/wiki/Data_scraping\" title=\"Data scraping\">data scraping</a> used for <a href=\"/wiki/Data_extraction\" title=\"Data extraction\">extracting data</a> from <a href=\"/wiki/Website\" title=\"Website\">websites</a>.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">[1]</a></sup> Web scraping software may directly access the <a href=\"/wiki/World_Wide_Web\" title=\"World Wide Web\">World Wide Web</a> using the <a href=\"/wiki/Hypertext_Transfer_Protocol\" class=\"mw-redirect\" title=\"Hypertext Transfer Protocol\">Hypertext Transfer Protocol</a> or a web browser. While web scraping can be done manually by a software user, the term typically refers to automated processes implemented using a <a href=\"/wiki/Internet_bot\" title=\"Internet bot\">bot</a> or <a href=\"/wiki/Web_crawler\" title=\"Web crawler\">web crawler</a>. It is a form of copying in which specific data is gathered and copied from the web, typically into a central local <a href=\"/wiki/Database\" title=\"Database\">database</a> or spreadsheet, for later <a href=\"/wiki/Data_retrieval\" title=\"Data retrieval\">retrieval</a> or <a href=\"/wiki/Data_analysis\" title=\"Data analysis\">analysis</a>.\\n</p>'"},"metadata":{}}]},{"cell_type":"markdown","source":"top 5 p tags using cssLocator","metadata":{}},{"cell_type":"code","source":"cssLocator = 'p'\np_tags_using_css_locator = sel.css(cssLocator)\np_tags_using_css_locator = p_tags_using_css_locator[:5]\np_tags_using_css_locator","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.89132Z","iopub.execute_input":"2024-07-14T22:58:35.892017Z","iopub.status.idle":"2024-07-14T22:58:35.902084Z","shell.execute_reply.started":"2024-07-14T22:58:35.891976Z","shell.execute_reply":"2024-07-14T22:58:35.900914Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"[<Selector query='descendant-or-self::p' data='<p><b>Web scraping</b>, <b>web harves...'>,\n <Selector query='descendant-or-self::p' data='<p>Scraping a web page involves fetch...'>,\n <Selector query='descendant-or-self::p' data='<p>As well as <a href=\"/wiki/Contact_...'>,\n <Selector query='descendant-or-self::p' data='<p><a href=\"/wiki/Web_page\" title=\"We...'>,\n <Selector query='descendant-or-self::p' data='<p>Newer forms of web scraping involv...'>]"},"metadata":{}}]},{"cell_type":"code","source":"p_tags_using_css_locator[1].extract()","metadata":{"execution":{"iopub.status.busy":"2024-07-14T22:58:35.90354Z","iopub.execute_input":"2024-07-14T22:58:35.903875Z","iopub.status.idle":"2024-07-14T22:58:35.91235Z","shell.execute_reply.started":"2024-07-14T22:58:35.903846Z","shell.execute_reply":"2024-07-14T22:58:35.911193Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'<p>Scraping a web page involves fetching it and extracting from it. Fetching is the downloading of a page (which a browser does when a user views a page). Therefore, web crawling is a main component of web scraping, to fetch pages for later processing. Once fetched, extraction can take place. The content of a page may be <a href=\"/wiki/Parsing\" title=\"Parsing\">parsed</a>, searched and reformatted, and its data copied into a spreadsheet or loaded into a database. Web scrapers typically take something out of a page, to make use of it for another purpose somewhere else. An example would be finding and copying names and telephone numbers, companies and their URLs, or e-mail addresses to a list (contact scraping).\\n</p>'"},"metadata":{}}]},{"cell_type":"markdown","source":"Applying web-scrapping on pakwheels","metadata":{}},{"cell_type":"code","source":"import requests\nurl = 'https://www.pakwheels.com/used-cars/automatic/57336'\nhtml = requests.get(url).content\nsel = Selector(text = html)","metadata":{"execution":{"iopub.status.busy":"2024-08-19T17:51:17.401216Z","iopub.execute_input":"2024-08-19T17:51:17.40186Z","iopub.status.idle":"2024-08-19T17:51:19.102763Z","shell.execute_reply.started":"2024-08-19T17:51:17.401825Z","shell.execute_reply":"2024-08-19T17:51:19.101037Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"cssLocator = 'ul.list-unstyled.search-results li div.col-md-9.grid-style div a h3::text'\ncar_titles = sel.css(cssLocator).getall()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T18:15:36.415738Z","iopub.execute_input":"2024-08-19T18:15:36.416202Z","iopub.status.idle":"2024-08-19T18:15:36.435559Z","shell.execute_reply.started":"2024-08-19T18:15:36.416168Z","shell.execute_reply":"2024-08-19T18:15:36.434475Z"},"scrolled":true,"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"car_titles","metadata":{"execution":{"iopub.status.busy":"2024-08-19T18:15:45.183754Z","iopub.execute_input":"2024-08-19T18:15:45.184194Z","iopub.status.idle":"2024-08-19T18:15:45.193445Z","shell.execute_reply.started":"2024-08-19T18:15:45.184163Z","shell.execute_reply":"2024-08-19T18:15:45.191961Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['Honda Civic  2018 Oriel 1.8 i-VTEC CVT for Sale',\n 'Toyota C-HR  2019 G-LED for Sale',\n 'Honda Accord  2004 CL7 for Sale',\n 'Toyota Fortuner  2022 Legender  for Sale',\n 'Mercedes Benz C Class  2012 C200 for Sale',\n 'Toyota Corolla  2014 GLi Automatic 1.6 VVTi for Sale',\n 'Honda N One  2021 Premium Tourer for Sale',\n 'Audi Q7  2010 3.6 FSI for Sale',\n 'Toyota Fortuner  2020 2.8 Sigma 4 for Sale',\n 'Toyota Prado  2018 TZ-G 2.8L Diesel for Sale',\n 'Toyota Prius  2007 S Touring Selection 1.5 for Sale',\n 'Honda Civic  2015  for Sale',\n 'Toyota Fortuner  2018 2.8 Sigma 4 for Sale',\n 'Honda Vezel  2014 Hybrid Z for Sale',\n 'Toyota Corolla Axio  2015 Hybrid 1.5 for Sale',\n 'Toyota Camry XV40 2007 Up-Spec Automatic 2.4 for Sale',\n 'Suzuki Ciaz  2017 Automatic for Sale',\n 'Toyota Corolla  2014 GLi Automatic 1.6 VVTi for Sale',\n 'Toyota Passo  2011 Plus Hana C for Sale',\n 'Toyota Prius  2007 G Touring Selection Leather Package 1.5 for Sale',\n 'Daihatsu Mira  2008 Custom X for Sale',\n 'Toyota Corolla  2016 Altis CVT-i 1.8 for Sale',\n 'Mercedes Benz E Class  1987  for Sale',\n 'Toyota Belta  2006 X Business A Package 1.0 for Sale',\n 'Suzuki Alto  2021 VXL AGS for Sale',\n 'Daihatsu Mira  2022 G SA III for Sale',\n 'Honda BR-V  2018 i-VTEC for Sale',\n 'Honda City  2020 1.3 i-VTEC Prosmatec for Sale',\n 'Toyota Fortuner  2021 2.7 V for Sale',\n 'Toyota C-HR  2018 G-LED for Sale',\n 'Honda Freed  2013  for Sale',\n 'Toyota Passo  2014 X L Package for Sale',\n 'Honda Civic  2018 Oriel 1.8 i-VTEC CVT for Sale']"},"metadata":{}}]}]}